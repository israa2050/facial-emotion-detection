{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israa2050/facial-emotion-detection/blob/main/Face%20Emotion%20Detection/Face_Emotion_Detection(WebCam_Video).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdcce08",
      "metadata": {
        "id": "dfdcce08"
      },
      "source": [
        "### Here we open webcam and test the two model Yolov5 for object detection and DL model for Emotion Detection the recorded video link below\n",
        "https://drive.google.com/file/d/1VZWA4k_omk6yB1_BtVctDfNnknQxo-P_/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import torch"
      ],
      "metadata": {
        "id": "5zgHXaZM-6su"
      },
      "id": "5zgHXaZM-6su",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3e2ad7",
      "metadata": {
        "id": "2b3e2ad7"
      },
      "outputs": [],
      "source": [
        "# Yolov5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='./best.pt') \n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "# Emotion model \n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "# load weights into new model\n",
        "emotion_model= tf.keras.models.load_model(\"./emotionl.h5\",custom_objects={'my_custom_loss': tfa.metrics.F1Score})\n",
        "print(\"Loaded model from disk\")\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "# start the webcam feed\n",
        "cap = cv2.VideoCapture(0)\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "# pass here your video path\n",
        "# cap = cv2.VideoCapture(\"upload your video\")\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.resize(frame, (1280, 720))\n",
        "    if not ret:\n",
        "        break\n",
        "    results = model(frame)\n",
        "    crops = results.crop(save=True, save_dir='runs/detect/exp')  # specify save dir\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "   # Get the detections as a list of dictionaries\n",
        "    detections = results.pandas().xyxy[0].to_dict(orient='records')\n",
        "    # Draw bounding boxes around the detections\n",
        "    for detection in detections:\n",
        "        x1, y1, x2, y2 = int(detection['xmin']), int(detection['ymin']), int(detection['xmax']), int(detection['ymax'])\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "       # Get coropped images\n",
        "        gray_img = cv2.cvtColor(crops[detections.index(detection)][\"im\"], cv2.COLOR_RGB2GRAY)\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(gray_img, (48, 48)), -1), 0)\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "        # predict the emotions\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x1+5, y1-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "    #show Image\n",
        "    cv2.imshow('frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}